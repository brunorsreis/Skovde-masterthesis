#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#B. Source Code of the AI-Enhanced Forensic Tool with Oracle Cloud and Cohere LLM
import os, re, json, logging, warnings
import warnings
warnings.filterwarnings("ignore", message=".*missing ScriptRunContext.*")
import pdfplumber, pandas as pd, chardet
from sentence_transformers import SentenceTransformer
from docx import Document
import oracledb, oci
from oci.generative_ai_inference import GenerativeAiInferenceClient
from oci.generative_ai_inference.models import (
    ChatDetails,
    OnDemandServingMode,
    CohereChatRequest
)
import streamlit as st
import numpy as np

# --- Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore")

# --- Safety Helper ---
def ensure_string(value):
    if isinstance(value, str):
        return value
    elif value is None:
        return ""
    else:
        return str(value)

# --- Load Embedder ---
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# --- Oracle DB Connection ---
conn = oracledb.connect(user="*****", password="****", dsn="localhost:1522/FREEPDB1")
#Replace *** with your user and password
cur = conn.cursor()
cur.execute("BEGIN EXECUTE IMMEDIATE 'DROP TABLE forensic_vectors'; EXCEPTION WHEN OTHERS THEN NULL; END;")
cur.execute("""
CREATE TABLE forensic_vectors (
    id NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY,
    filename VARCHAR2(255),
    content CLOB,
    embedding CLOB
)
""")

# --- File Reading ---
def read_file_content(file):
    ext = os.path.splitext(file.name)[1].lower()
    try:
        if ext == ".pdf":
            with pdfplumber.open(file) as pdf:
                return ensure_string("\n".join(ensure_string(p.extract_text()) for p in pdf.pages))

        elif ext in [".txt", ".log", ".md"]:
            raw = file.read()
            encoding = chardet.detect(raw).get("encoding") or "utf-8"
            return ensure_string(raw.decode(encoding, errors='ignore'))

        elif ext == ".csv":
            return ensure_string(pd.read_csv(file).to_string(index=False))

        elif ext in [".xls", ".xlsx"]:
            return ensure_string(pd.read_excel(file).to_string(index=False))

        elif ext == ".json":
            return ensure_string(json.dumps(json.load(file), indent=2))

        elif ext == ".docx":
            return ensure_string("\n".join(ensure_string(p.text) for p in Document(file).paragraphs))

        else:
            return ensure_string(file.read().decode('utf-8', errors='ignore'))

    except Exception as e:
        return ensure_string(f"‚ùå Error reading {file.name}: {e}")

# --- Sensitive Info Detection ---
def detect_sensitive_info(text):
    patterns = {
        "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b',
        "phone": r'\b\+?\d{1,3}?[-.\s]??\(?\d{2,5}\)?[-.\s]?\d{3,5}[-.\s]?\d{3,5}\b',
        "credit_card": r'\b(?:\d[ -]*?){13,16}\b',
        "password": r'(?i)\bpassword\s*[:=]\s*\S+\b',
        "ip_address": r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
    }

    sensitive_data = []
    for label, pattern in patterns.items():
        try:
            matches = list(set(re.findall(pattern, text)))
            if matches:
                sensitive_data.append((label, matches))
        except Exception as e:
            logger.warning(f"Regex failed for {label}: {e}")
    return sensitive_data

# --- Oracle LLM Integration (Chat-based with working Cohere model) ---
def oracle_llm(text, compartment_id):
    try:
        config = oci.config.from_file()
        client = GenerativeAiInferenceClient(config)

        # OCI Model ID (Cohere Command A - Working Chat Model)
        model_id = "******"
        #Replace ***** from the information of the model_id (Hint Generative AI tab inside the Oracle Cloud OCI)

        chat_request = CohereChatRequest(
            message=(
                "You are a cybersecurity forensic AI. Analyze the following:\n"
                "1. Identify sensitive data (emails, IPs, passwords, credit cards).\n"
                "2. Assess exposure severity.\n"
                "3. Suggest mitigations.\n\n"
                f"{text[:4000]}"
            ),
            temperature=0.5,
            top_p=0.9,
            top_k=0,
            max_tokens=500,
            frequency_penalty=0
        )

        chat_details = ChatDetails(
            compartment_id=compartment_id,
            serving_mode=OnDemandServingMode(model_id=model_id),
            chat_request=chat_request
        )

        response = client.chat(chat_details)
        return response.data.chat_response.text

    except Exception as e:
        return f"‚ùå Oracle LLM Error: {e}"

# --- Streamlit UI ---
st.set_page_config(page_title="Oracle Cyber Forensic Analyzer")
st.title("üîê Oracle Cyber Forensic Analyzer")
st.caption("Upload files (PDF, DOCX, TXT, CSV, JSON). Detect sensitive data and summarize using Oracle LLM.")

uploaded_files = st.file_uploader("üìÇ Upload Files", accept_multiple_files=True)
compartment_id = st.text_input("üîê OCI Compartment OCID")

if st.button("Submit") and uploaded_files and compartment_id:
    with st.spinner("Analyzing..."):
        results = []

        for file in uploaded_files:
            filename = file.name
            st.info(f"üîç Processing: {filename}")
            text = read_file_content(file)

            try:
                embedding = embedder.encode(text)
                emb_json = json.dumps(embedding.tolist())
            except Exception as e:
                emb_json = "[]"
                logger.error(f"Embedding failed: {e}")

            sensitive = detect_sensitive_info(text)

            try:
                cur.execute("""
                    INSERT INTO forensic_vectors (filename, content, embedding)
                    VALUES (:fn, :ct, :em)
                """, {"fn": filename, "ct": text, "em": emb_json})
                conn.commit()
                db_status = "üü¢ Stored in DB."
            except Exception as e:
                db_status = f"‚ùå DB Error: {e}"

            sensitive_str = "\n".join(f"{k}: {', '.join(v)}" for k, v in sensitive) or "No sensitive data found."
            llm_result = oracle_llm(text, compartment_id)

            st.success(f"üìÑ File: {filename}")
            st.markdown(f"**{db_status}**")
            st.markdown("### üîç Sensitive Info")
            st.code(sensitive_str)
            st.markdown("### üß† LLM Summary")
            st.code(llm_result)

